{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested with `keras==3.1.1`, `tensorflow==2.16.1`, `numpy==1.26.4`, `scikit-learn==1.5.2`, `pandas==2.2.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\"songs\", \"covertype\", \"nids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "\n",
    "if dataset_name == \"songs\":\n",
    "    # https://www.kaggle.com/datasets/amitanshjoshi/spotify-1million-tracks/data\n",
    "    full_dataset = pd.read_csv(\"data/spotify_data.csv\", index_col=0)\n",
    "    full_dataset.rename(columns={\"genre\": \"label\"}, inplace=True)\n",
    "    full_dataset[\"label\"] = sklearn.preprocessing.LabelEncoder().fit_transform(full_dataset[\"label\"])\n",
    "    X = full_dataset[[\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\", \"time_signature\"]]\n",
    "    y = full_dataset[\"label\"]\n",
    "elif dataset_name == \"covertype\":\n",
    "    # https://archive.ics.uci.edu/dataset/31/covertype\n",
    "    full_dataset = sklearn.datasets.fetch_covtype(as_frame=True).frame\n",
    "    full_dataset.rename(columns={\"Cover_Type\": \"label\"}, inplace=True)\n",
    "    full_dataset[\"label\"] = full_dataset[\"label\"] - 1\n",
    "    X = full_dataset.drop(columns=[\"label\"])\n",
    "    y = full_dataset[\"label\"]\n",
    "elif dataset_name == \"nids\":\n",
    "    # https://rdm.uq.edu.au/files/2ad93cd0-ef9c-11ed-827d-e762de186848\n",
    "    # https://staff.itee.uq.edu.au/marius/NIDS_datasets/ \n",
    "    full_dataset = pd.read_csv(\"data/NF-ToN-IoT.csv\").drop(columns=[\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", \"Label\"])\n",
    "    full_dataset.rename(columns={\"Attack\": \"label\"}, inplace=True)\n",
    "    X = full_dataset.drop(columns=[\"label\"])\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(full_dataset[\"label\"])\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset name\")\n",
    "\n",
    "value_counts = full_dataset[\"label\"].value_counts()\n",
    "frequecies = value_counts / value_counts.sum()\n",
    "entropy = -np.sum(frequecies * np.log2(frequecies))\n",
    "classes = len(value_counts)\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Entropy: {entropy:.4f}\")\n",
    "\n",
    "model_path = f\"models/{dataset_name}_models/\"\n",
    "full_dataset[\"label\"].value_counts()#, full_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import struct\n",
    "\n",
    "sc = sklearn.preprocessing.StandardScaler()\n",
    "#sc = sklearn.preprocessing.MinMaxScaler()\n",
    "Xt = sc.fit_transform(X).astype(\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "with open(f\"data/{dataset_name}_X.lrbin\", \"wb\") as f:\n",
    "    f.write(struct.pack(\"<Q\", Xt.shape[0]))\n",
    "    f.write(struct.pack(\"<Q\", Xt.shape[1]))\n",
    "    Xt.tofile(f)\n",
    "\n",
    "np.insert(y, 0, classes).astype(np.uint16).tofile(f\"data/{dataset_name}_y.lrbin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "import keras\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "hidden_units = 50\n",
    "num_layers = 1\n",
    "model_name = f\"{dataset_name}_mlp_L{num_layers}_H{hidden_units}\"\n",
    "filename = f\"{model_name}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],), name=\"input\"),\n",
    "    *[keras.layers.Dense(hidden_units, activation=\"relu\") for _ in range(num_layers)],\n",
    "    keras.layers.Dense(classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_sparse_categorical_accuracy\",\n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(),\n",
    "        keras.metrics.SparseTopKCategoricalAccuracy(3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(filename)\n",
    "model.summary()\n",
    "\n",
    "train_start = time.perf_counter()\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[es],\n",
    ")\n",
    "train_end = time.perf_counter()\n",
    "print(f\"Training time: {train_end - train_start} s\")\n",
    "\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "model.save(f\"{model_path}/{filename}.keras\")\n",
    "with open(f\"{model_path}/{filename}_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history | {\"training_seconds\": train_end - train_start}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_start = time.perf_counter()\n",
    "benchmarks = model.evaluate(X_test, y_test, return_dict=True, verbose=0)\n",
    "eval_end = time.perf_counter()\n",
    "\n",
    "print(benchmarks)\n",
    "print(f\"{1_000_000 * (eval_end - eval_start) / len(y_test)} µs/sample\")\n",
    "\n",
    "with open(f\"{model_path}/{filename}_eval.pkl\", \"wb\") as f:\n",
    "    pickle.dump(benchmarks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_entropy = 0\n",
    "correct_predictions = 0\n",
    "all_probs = np.concatenate([model.predict(X_test, verbose=0), model.predict(X_train, verbose=0)], axis=0)\n",
    "all_labels = np.concatenate([y_test, y_train], axis=0)\n",
    "for freqs, label in zip(all_probs, all_labels):\n",
    "    correct_predictions += 1 if label == freqs.argmax() else 0\n",
    "    model_entropy -= np.log2(freqs[label])\n",
    "model_entropy /= len(full_dataset)\n",
    "\n",
    "print(\"Overall accuracy:\", correct_predictions / len(full_dataset))\n",
    "print(\"Model entropy:\", model_entropy)\n",
    "print(\"Model params:\", model.count_params())\n",
    "print(\"Model bytes:\", os.path.getsize(f\"{model_path}/{filename}.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.lite.python import interpreter\n",
    "\n",
    "def representative_dataset():\n",
    "  for x in X_train[:1000]:\n",
    "    yield {\"input\": x}\n",
    "\n",
    "quantized_modes = []\n",
    "\n",
    "for quantization in [None, \"uint8\", \"float16\"]:\n",
    "    print(\"#\" * 35, f\"{quantization} quantization\", \"#\" * 35)\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "    model = keras.saving.load_model(f\"{model_path}/{filename}.keras\", compile=False)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    if quantization:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        if quantization == \"uint8\":\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "            converter.representative_dataset = representative_dataset\n",
    "            converter.inference_input_type = tf.float32\n",
    "            converter.inference_output_type = tf.uint8\n",
    "        elif quantization == \"float16\":\n",
    "            converter.target_spec.supported_types = [tf.float16]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid quantization type\")\n",
    "        \n",
    "    tflite_model = converter.convert()\n",
    "    suffix = f\"_{quantization}\" if quantization else \"\"\n",
    "    with open(f\"{model_path}/{filename}{suffix}.tflite\", \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    quantized_modes.append(tflite_model)\n",
    "\n",
    "for quantization, tflite_model in zip([\"none\", \"uint8\", \"float16\"], quantized_modes):\n",
    "    interp = interpreter.InterpreterWithCustomOps(model_content=tflite_model)\n",
    "    input_details = interp.get_input_details()\n",
    "    output_details = interp.get_output_details()\n",
    "    input_type = input_details[0]['dtype']\n",
    "    tflite_X = X_train\n",
    "    # if input_type == np.uint8:\n",
    "    #     input_scale, input_zero_point = input_details[0]['quantization']\n",
    "    #     print(f\"Input scale: {input_scale}, zero point: {input_zero_point}\")\n",
    "    #     tflite_X = np.around((X_train / input_scale) + input_zero_point).astype(np.uint8)\n",
    "\n",
    "    interp.allocate_tensors()\n",
    "    model_runner = interp.get_signature_runner(\"serving_default\")\n",
    "    t0 = time.perf_counter()\n",
    "    tflite_y = model_runner(input=tflite_X)\n",
    "    t1 = time.perf_counter()\n",
    "    tflite_y = list(tflite_y.values())[0]\n",
    "    accuracy = keras.metrics.sparse_categorical_accuracy(y_train, tflite_y).numpy().mean()\n",
    "    print(f\"{quantization} quantization bytes: {len(tflite_model)}\")\n",
    "    print(f\"{quantization} quantization µs/prediction: {1_000_000 * (t1 - t0) / X_train.shape[0]}\")\n",
    "    print(f\"{quantization} quantization accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
