{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers --quiet\n",
    "!pip install ipywidgets --user --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60774784",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aeb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "# v177\n",
    "dataset = load_dataset(\"arxiv_dataset\", data_dir=\"datasets\", split=\"train\", trust_remote_code=True, verification_mode=\"no_checks\")#.select(range(100000)) \n",
    "\n",
    "def keep_first_arxiv_category(example):\n",
    "    example[\"category\"] = example[\"categories\"].split(' ', 1)[0]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(keep_first_arxiv_category)\n",
    "dataset = dataset.class_encode_column(\"category\")\n",
    "dataset = dataset.remove_columns([\"id\", \"submitter\", \"authors\", \"comments\", \"journal-ref\", \"doi\",\n",
    "                                  \"report-no\", \"categories\", \"license\", \"abstract\", \"update_date\"])\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = dataset['train'].features['category'].num_classes\n",
    "id2label = dict(enumerate(dataset['train'].features['category'].names))\n",
    "label2id = {val: key for key, val in id2label.items()}\n",
    "\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 200\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def get_tf_dataset(split):\n",
    "    train_features = tokenizer(dataset[split][\"title\"], max_length=MAX_LEN, truncation=True, padding=True)\n",
    "    return tf.data.Dataset.from_tensor_slices((dict(train_features), dataset[split][\"category\"])).batch(batch_size)\n",
    "\n",
    "train_tf_dataset = get_tf_dataset(\"train\")\n",
    "test_tf_dataset = get_tf_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c2158",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=num_labels, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9337cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, strftime\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              steps_per_execution=100,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "filename = \"distilbert_arxiv_\" + strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(filename)\n",
    "\n",
    "start = time()\n",
    "history = model.fit(train_tf_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs, initial_epoch=1)\n",
    "end = time()\n",
    "print(end-start)\n",
    "\n",
    "model.save_pretrained(f\"arxiv_model/{filename}\")\n",
    "with open(f\"arxiv_model/{filename} info.pkl\", \"wb\") as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN, start, end), f)\n",
    "with open(f\"arxiv_model/{filename} history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bca6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = model.evaluate(test_tf_dataset, return_dict=True, batch_size=batch_size)\n",
    "print(benchmarks)\n",
    "\n",
    "with open(f\"arxiv_model/{filename} eval.pkl\", \"wb\") as f:\n",
    "    pickle.dump(benchmarks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d0df9",
   "metadata": {},
   "source": [
    "# Write examples, labels, and predictions to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples = dataset[\"train\"][\"title\"] + dataset[\"test\"][\"title\"]\n",
    "all_examples_tf_dataset = tf.data.Dataset.from_tensor_slices(dict(tokenizer(all_examples, max_length=MAX_LEN, truncation=True, padding=True))).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_start = time()\n",
    "preds = model.predict(all_examples_tf_dataset).logits\n",
    "pred_end = time()\n",
    "print(pred_end - pred_start, \"seconds\")\n",
    "\n",
    "preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_labels = dataset[\"train\"][\"category\"] + dataset[\"test\"][\"category\"]\n",
    "df = pd.concat([pd.DataFrame({'example': all_examples, 'label': all_labels}),\n",
    "                pd.DataFrame(data=preds, columns=dataset['train'].features['category'].names)], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"arxiv_model/{filename} outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correct = (preds.argmax(axis=1) == np.array(all_labels))\n",
    "correct_train = correct[:len(dataset[\"train\"])].sum()\n",
    "correct_test = correct[len(dataset[\"train\"]):].sum()\n",
    "print(\"Train accuracy\", correct_train / len(dataset[\"train\"]), correct_train)\n",
    "print(\"Test accuracy\", correct_test / len(dataset[\"test\"]), correct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce628a",
   "metadata": {},
   "source": [
    "# Query the model interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b093f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "pipe(\"Learned retrieval data structures\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38202215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
